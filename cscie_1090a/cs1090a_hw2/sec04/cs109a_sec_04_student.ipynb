{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usv5O_eL9m3W"
      },
      "source": [
        "# CS1090A Introduction to Data Science\n",
        "\n",
        "## Section 4: Polynomial Regression, Regularization, and Cross Validation\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Fall 2025**<br/>\n",
        "**Instructors**: Pavlos Protopapas and Kevin Rader<br/>\n",
        "**Preceptor**: Chris Gumb\n",
        "<hr style='height:2px'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bUGxYJP9m3Y"
      },
      "outputs": [],
      "source": [
        "# Data and Stats packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Intelligence packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_6v9REG8j6d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !curl -L \"https://drive.google.com/uc?export=download&id=1nUY21Ra9hTCiqpum6bg9JYjGox7JuwZA\" -o /content/Data.zip\n",
        "    !unzip -o /content/Data.zip -d /content\n",
        "elif not os.path.exists('Data'):\n",
        "    !curl -L \"https://drive.google.com/uc?export=download&id=1nUY21Ra9hTCiqpum6bg9JYjGox7JuwZA\" -o Data.zip\n",
        "    !unzip -o Data.zip\n",
        "    !rm Data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY5FQxOc7hz2"
      },
      "source": [
        "### Contents:\n",
        "TOTAL 75 mins\n",
        "* EDA & Feature Engineering (15 min)\n",
        "* Polynomial Regression & Hyperparameter Tuning (15 min)\n",
        "* Cross-Validation for Robust Model Selection (20 min)\n",
        "* Regularization for Overfitting (25 min)\n",
        "  * Ridge (L2) and Lasso (L1)\n",
        "  * Visualizing Model Performance and Coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjPbhKS69m3Z"
      },
      "source": [
        "<a id=intro></a>\n",
        "\n",
        "## Question - Can we predict the \"market value\" of prospective players in the *Fantasy Premier League* dataset?\n",
        "![trent](https://drive.google.com/uc?id=1B4drp0SEHM_9hXtzlyszak_fTIvlecxe)\n",
        "\n",
        "\n",
        "<p style=\"text-align:right\"><font size=\"1\"; text-align='right'>(Image: Trent Alexander-Arnold https://www.premierleague.com/news/2766220)</font></p>\n",
        "\n",
        "\n",
        "### Story\n",
        "Once upon a time, the Football Association gave us a dataset and asked us to help them predict the <b>market value</b> of prospective players.\n",
        "\n",
        "### The dataset\n",
        "The dataset includes data up to 2017, and was created by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) who used a variety of sources, including *transfermrkt.com* and *Fantasy Premier League (FPL)*, and a variety of methods, including scraping.\n",
        "Each observation is a collection of facts about players in the English Premier League.\n",
        "\n",
        "---\n",
        "\n",
        "`name`             : Name of the player  \n",
        "`club`             : Club of the player  \n",
        "`age`              : Age of the player  \n",
        "`position`         : The usual position on the pitch  \n",
        "`position_cat`     : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers   \n",
        "`page_views`       : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
        "`fpl_points`       : FPL points accumulated over the previous season (https://www.premierleague.com/news/2174909)<BR>\n",
        "`region`           : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
        "`nationality`      : Player's nationality <BR>\n",
        "`new_signing`      : Whether a new signing for 2017/18 (till 20th July)  \n",
        "`new_foreign`      : Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
        "`club_id`          : a numerical version of the Club feature\n",
        "  \n",
        "---\n",
        "\n",
        "### Our goal\n",
        "To construct and fit a model that predicts the players' `market value` using all or part of the features in the given data.\n",
        "\n",
        "### Our return variable\n",
        "\n",
        "`market_value`: As on *transfermrkt.com* on July 20th, 2017 in Â£M (British pounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zjj5e0ZROn9"
      },
      "source": [
        "## 1 - EDA, preliminary feature selection and engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iz-NbNA9m3a"
      },
      "source": [
        "### Import the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEYd-MpB9m3a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Data/league_data.csv\")\n",
        "df.describe(include=\"all\").T # All columns of the input (including categorical variables) will be included in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "632PmyLo9m3b"
      },
      "source": [
        "### Any missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EKjSK0c9m3b"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZheHP-ScKOgq"
      },
      "source": [
        "Nope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh6o7WgD9m3b"
      },
      "source": [
        "### Features selection and some data cleaning\n",
        "    \n",
        "This should be a preliminary check for obvious features to include or excude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_UfDK-fGbPk"
      },
      "source": [
        "> **Probing into the variables.**\n",
        ">- `name` should not affect the market values.  \n",
        ">- `club`, `age`, `position`, `position_cat`, `page_views`, `fpl_points`, `region`, `nationality`,`new_signing`, `new_foreign`, and `club_id` are *likely* to affect the market values.\n",
        ">- `position` and `position_cat` may contain similar information.\n",
        ">- `region`, `nationality`, and `new_foreign` and may contain similar information.\n",
        ">- `club` and `club_id` may contain similar information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrqLOpqqNrnV"
      },
      "source": [
        "#### `position` vs `position_cat`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-Qx3gUh9m3b"
      },
      "outputs": [],
      "source": [
        "# How many unique positions are there?\n",
        "df.position.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ8jghvlE0nF"
      },
      "source": [
        "If we use all these positions, too many new features will be generated after one-hot encoding.\n",
        "\n",
        "> **Spoiler alert:** Too many features may not be too constructive to models.\n",
        "\n",
        "We can simply use `position_cat`, which groups different specific positions into general ones: 1. attackers, 2. midfielder, 3. defender, and 4. goalkeeper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb4rDS0Z9m3b"
      },
      "outputs": [],
      "source": [
        "# What is the relationship between the `position_cat` and `position` features?\n",
        "df.groupby(['position_cat']).agg({'position': np.unique})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8t0xWmAN-wc"
      },
      "source": [
        "#### `region` vs `nationality`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ0RxegP9m3b"
      },
      "outputs": [],
      "source": [
        "# Similarly, what is the relationship between `region` and `nationality`?\n",
        "regions = df.groupby(['region']).agg({'nationality' : np.unique})\n",
        "regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4PZTg3VFx7S"
      },
      "source": [
        "Some nationality is included in multiple regions (e.g. Argentina). Let's find all such nationality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWytXBVYEgQG"
      },
      "outputs": [],
      "source": [
        "for i in regions.index:\n",
        "    region_a = set(regions.loc[i].values[0])\n",
        "    for j in regions.index:\n",
        "        region_b = set(regions.loc[j].values[0])\n",
        "        if j > i:\n",
        "            intersection = set.intersection(region_a, region_b)\n",
        "            if len(intersection) > 0:\n",
        "                print(f'Nationality occurs in both Region {i} and {j}:, {intersection}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_WemZRHGAXg"
      },
      "source": [
        "Let's clean up the regions by including the same nationality in ***one and the same*** region, generating a new feature called `region_organized`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KksE0AmO5w_A"
      },
      "outputs": [],
      "source": [
        "from Data.helper import reorganize_regions # A function to strictly rearrange the region according to nationality\n",
        "\n",
        "# Create 'region_organized' feature\n",
        "df['region_organized'] = df[['region', 'nationality']].apply(reorganize_regions, axis=1)\n",
        "\n",
        "# Create 'big_six' feature\n",
        "big_six = ['Arsenal', 'Chelsea','Liverpool', 'Manchester+City','Manchester+United', 'Tottenham']\n",
        "df['big_six'] = df.club.isin(big_six).astype(int)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8rRjykRqn35"
      },
      "source": [
        "> **Mini invitation (Feel free to skip)**: If curious, please study (or reflect on) what `pandas.DataFrame.apply()` is doing at [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html). If not trusting us, feel free to see what `reorganize_regions()` from `Data/helper.py` is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOmIWP1D9m3c"
      },
      "source": [
        "> **Question:** Is being in the *big six* affect market value?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qnxVIBu2TpI"
      },
      "outputs": [],
      "source": [
        "ax = sns.histplot(data=df, x='market_value', hue='big_six', stat='proportion', kde=True)\n",
        "ax.set_title(\"Does being in a big club affect market value?\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfS1HqVZ9m3c"
      },
      "source": [
        " ### Train-test split (with stratification)\n",
        "\n",
        "![test](https://drive.google.com/uc?id=1mgsEfUb87mTt3NXwsIZdzfxVyPSseipP)\n",
        "\n",
        "*Just for now*, let us first divide the dataset into train and test sets, as usual.\n",
        "\n",
        "We want to make sure that the training and test data have appropriate representation of certain variables; it would be bad for the training data or the test data to entirely miss a region, for example. This is especially important when some features are underrepresented (our data is **imbalanced**). To see the numbers in each category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD2HUQ5Y6hfC"
      },
      "outputs": [],
      "source": [
        "print(df['region_organized'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7l_oL656tjJ"
      },
      "source": [
        "We can fix this issue with statification to make sure the splitted datasets have similar distribution in terms of `region_organized`. To stratify, we set the parameter `stratify = df['region_organized']`\n",
        "\n",
        "> **Note:** This will not work if the dataset contains missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1XPpo0eR6pi"
      },
      "outputs": [],
      "source": [
        "# Designate predictor variables\n",
        "categorical_cols = ['position_cat', 'new_signing', 'region_organized', 'big_six']\n",
        "numerical_cols = ['page_views', 'fpl_points', 'age']\n",
        "ordinal_cols = [] # we do not have any\n",
        "\n",
        "# Designate response variables\n",
        "response = 'market_value'\n",
        "y = df[response]\n",
        "x = df.drop(response, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EGkc6Fv9m3d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "                                            x, y,\n",
        "                                            random_state=109,\n",
        "                                            stratify=df['region_organized'],\n",
        "                                            test_size = 0.2,\n",
        "                                            )\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOCabVu5yZlL"
      },
      "source": [
        "> **Question:** We used `stratify=df['region_organized']` when splitting our data. What problem does stratification solve, especially for a feature like `region_organized` where some categories have very few samples? What might happen if we didn't stratify?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAyX7Iw9m3d"
      },
      "source": [
        "## 2 - Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OzJbsxI85Tr"
      },
      "source": [
        " ### Extract the chosen variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MneFhFsB9m3p"
      },
      "outputs": [],
      "source": [
        "print(f'Chosen categorical predictor variables: {categorical_cols}')\n",
        "print(f'Chosen numerical predictor variables: {numerical_cols}')\n",
        "cols = categorical_cols + numerical_cols\n",
        "\n",
        "X_train = x_train[cols].copy()\n",
        "X_test = x_test[cols].copy()\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdDdxLxi9m3q"
      },
      "source": [
        "### Transform categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEXdwhec9m3q"
      },
      "source": [
        "We will use sklearn's `OneHotEncoder()` for converting categorical variables (ordinal and non-ordinal ) to one-hot encoded ones.\n",
        "\n",
        "> **Note:** By default, it keeps all one-hot created columns. It also has a fine-grained drop mechanism.\n",
        "> - `drop=âfirstâ`: drop the first category in each feature.\n",
        ">\n",
        ">   - For example, consider we have four players (the four rows respectively corresponding to a attacker, a midfielder, a defender, and a goalkeeper) and multiple one hot encoded features (columns). If we drop the first feature (column), we will still have the four players represented uniquely:\n",
        ">\n",
        ">$$\n",
        "\\begin{align}\n",
        "\\begin{matrix}\n",
        "  Position\\ \\ \\ \\ \\ \\\n",
        "\\end{matrix} &\n",
        "\\begin{matrix}\n",
        "  Position\\ (dropped\\ first)\n",
        "\\end{matrix}       \\\\\n",
        "\\begin{matrix}\n",
        "  \\textbf{1} & \\textbf{2} & \\textbf{3} & \\textbf{4}\n",
        "\\end{matrix}\\ \\ \\ & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{matrix}\n",
        "  \\textbf{2} & \\textbf{3} & \\textbf{4}\n",
        "\\end{matrix} \\\\\n",
        "\\begin{matrix}\n",
        "An\\ attacker\\ player\\rightarrow \\\\\n",
        "A\\ midfielder\\ player \\rightarrow \\\\\n",
        "A\\ defender\\ player \\rightarrow \\\\\n",
        "A\\ goalkeeper\\ player \\rightarrow\n",
        "\\end{matrix}\\begin{pmatrix}\n",
        "  \\underline{1} & 0 & 0 & 0\\\\\n",
        "  \\underline{0} & 1 & 0 & 0\\\\\n",
        "  \\underline{0} & 0 & 1 & 0\\\\\n",
        "  \\underline{0} & 0 & 0 & 1\n",
        "\\end{pmatrix} &\\Rightarrow\\begin{pmatrix}\n",
        "  0 & 0 & 0\\\\\n",
        "  1 & 0 & 0\\\\\n",
        "  0 & 1 & 0\\\\\n",
        "  0 & 0 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "> >**Nested Note:** Dropping the first category makes the distance between variables no longer equally distanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyN_Y_C9m3q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Initialize and fit the encoder\n",
        "# sparse_output=False ensures the output is a dense numpy array, which is easier to work with in pandas.\n",
        "# The default is to produce a sparse matrix, which is memory-efficient when you have many categories.\n",
        "ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
        "ohe_train = ohe.fit_transform(X_train[categorical_cols])\n",
        "\n",
        "# Create a DataFrame with the one-hot encoded features\n",
        "ohe_train_df = pd.DataFrame(ohe_train, columns=ohe.get_feature_names_out())\n",
        "\n",
        "# Transform the test set and create a DataFrame\n",
        "ohe_test = ohe.transform(X_test[categorical_cols])\n",
        "ohe_test_df = pd.DataFrame(ohe_test, columns=ohe.get_feature_names_out())\n",
        "\n",
        "ohe_train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N28C_ypEyZlM"
      },
      "source": [
        "> **Question:** We used `drop='first'` when one-hot encoding. In the context of linear regression, what potential issue does this help us avoid?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN7aNF1J9m3q"
      },
      "source": [
        "**OK**, our categorical variables are one-hot encoded and ready in `ohe_train_df` and `ohe_test_df`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLctUSWp9m3q"
      },
      "source": [
        "### Transform numerical variables\n",
        "\n",
        "We will use sklearn's `StandardScaler`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHGMkI3i9m3q"
      },
      "outputs": [],
      "source": [
        "# scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform it\n",
        "scaled_train = ...\n",
        "# Transform the test set using the same scaler\n",
        "scaled_test = ...;\n",
        "\n",
        "# Create a DataFrame with the scaled features\n",
        "scaled_train_df = pd.DataFrame(scaled_train, columns=numerical_cols)\n",
        "scaled_test_df = pd.DataFrame(scaled_test, columns=numerical_cols);\n",
        "\n",
        "# Verify the scaling\n",
        "scaled_train_df.describe().head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmyhVOg4yZlO"
      },
      "source": [
        "> **Question:** Why is scaling numerical features important, especially in models that involve distances or penalties on coefficient sizes? What might happen if we have one feature measured in meters and another in millimeters?\n",
        "\n",
        "> **Gotcha!** A common mistake is to re-fit the scaler on the test data (e.g., by calling `scaler.fit_transform(X_test)`). Why is it critical to `fit` *only* on the training data, and then use that *same* fitted scaler to `transform` the test data? What would be the consequence of applying a different transformation (i.e., scaling based on different means and standard deviations) to the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAPFjYvG9m3r"
      },
      "source": [
        "**OK**, our numerical variables are scaled and ready in `scaled_train_df` and `scaled_test_df`. Our categorical variables are one-hot encoded and ready in `ohe_train_df` and `ohe_test_df`.\n",
        "\n",
        "Let's combine them into a single training and testing dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "combine-features"
      },
      "outputs": [],
      "source": [
        "# create the data frame for the train set\n",
        "X_train_df = pd.concat([scaled_train_df, ohe_train_df], axis=1)\n",
        "\n",
        "# create the data frame for the test set\n",
        "X_test_df = pd.concat([scaled_test_df, ohe_test_df], axis=1)\n",
        "\n",
        "X_train_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SyVcVy03Eaq"
      },
      "source": [
        "## 3 - Model Selection for Hyperparameters\n",
        "\n",
        "The process of choosing the best model from a set of candidate models is called **model selection**. The candidates can be different algorithms (e.g. Linear Regression vs. k-NN) or the same algorithm with different settings, called **hyperparameters**. For example, when using a k-NN model, `k` is a hyperparameter. You have already performed model selection in a previous section when you chose the best `k` for your k-NN model.\n",
        "\n",
        "For a linear regression model with polynomial features, the degree of the polynomial is a hyperparameter. We need to choose a degree that is complex enough to capture the underlying patterns in the data, but not so complex that it overfits to the noise.\n",
        "\n",
        "### Tuning with a Single Validation Set\n",
        "\n",
        "A simple and effective strategy for hyperparameter tuning is to split the training data further into a new, smaller training set and a **validation set**.\n",
        "\n",
        "- The model is trained on the new, smaller training set.\n",
        "- It is evaluated on the validation set.\n",
        "- The hyperparameter that gives the best performance on the validation set is chosen.\n",
        "- Finally, the model with the best hyperparameter is re-trained on the *entire* original training data and its final performance is reported on the held-out test set.\n",
        "\n",
        "Let's use this strategy to find the best polynomial degree.\n",
        "\n",
        "> **Gotcha!** When using `PolynomialFeatures`, it's often a good idea to set `include_bias=False`. This prevents `PolynomialFeatures` from adding a column of ones (the intercept term). Why? Because `scikit-learn`'s `LinearRegression` model will automatically add an intercept for us. Including it twice can be redundant and sometimes confusing for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new-train-val-split"
      },
      "outputs": [],
      "source": [
        "# Let's give our data more sensible names.\n",
        "X_train_val = X_train_df.copy()\n",
        "y_train_val = y_train.copy()\n",
        "\n",
        "# Split the data into train and validation sets.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
        "                                                  y_train_val,\n",
        "                                                  stratify=X_train_val[['region_organized_2','region_organized_3','region_organized_4']],\n",
        "                                                  train_size=0.8,\n",
        "                                                  random_state=42\n",
        "                                                  )\n",
        "\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6epoTvMyZlO"
      },
      "source": [
        "> **Question:** Why do we need a separate validation set to tune hyperparameters like the polynomial degree? What's wrong with picking the degree that gives the best score on our final test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tune-poly-degree"
      },
      "outputs": [],
      "source": [
        "degrees = [1, 2, 3, 4, 5]\n",
        "val_mses = []\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features for the current degree\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "\n",
        "    # We only want to create polynomial features for the numerical columns\n",
        "    # The OHE columns should be left as is\n",
        "    numerical_cols = scaled_train_df.columns\n",
        "\n",
        "    # Transform the numerical features\n",
        "    X_train_poly = poly.fit_transform(X_train[numerical_cols])\n",
        "    X_val_poly = poly.transform(X_val[numerical_cols])\n",
        "\n",
        "    # Get the names of the new polynomial features\n",
        "    poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
        "\n",
        "    # Create DataFrames with the new features\n",
        "    X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly_feature_names, index=X_train.index)\n",
        "    X_val_poly_df = pd.DataFrame(X_val_poly, columns=poly_feature_names, index=X_val.index)\n",
        "\n",
        "    # Combine with one-hot encoded features\n",
        "    ohe_cols = ohe_train_df.columns\n",
        "    X_train_full = pd.concat([X_train_poly_df, X_train[ohe_cols]], axis=1)\n",
        "    X_val_full = pd.concat([X_val_poly_df, X_val[ohe_cols]], axis=1)\n",
        "\n",
        "    # Fit a linear regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_full, y_train)\n",
        "\n",
        "    # Predict on the validation set and calculate MSE\n",
        "    y_val_pred = model.predict(X_val_full)\n",
        "    mse = mean_squared_error(y_val, y_val_pred)\n",
        "    val_mses.append(mse)\n",
        "\n",
        "# Find the best degree\n",
        "best_degree = ...\n",
        "print(f\"Validation MSEs: {val_mses}\")\n",
        "print(f\"Best polynomial degree: {best_degree}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-poly-degree-tuning"
      },
      "outputs": [],
      "source": [
        "plt.plot(degrees, val_mses, marker='o')\n",
        "plt.xlabel(\"Polynomial Degree\")\n",
        "plt.ylabel(\"Validation MSE\")\n",
        "plt.title(\"Validation MSE vs. Polynomial Degree\")\n",
        "plt.xticks(degrees)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv-section-start"
      },
      "source": [
        "## 4 - Robust Model Selection with Cross-Validation\n",
        "\n",
        "A single validation set can be effective, but its performance can be sensitive to which specific data points happen to end up in the training vs. validation split. A different random split could result in a different \"best\" hyperparameter.\n",
        "\n",
        "**Cross-validation** is a more robust technique that mitigates this randomness. In k-fold cross-validation, the dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance more reliably.\n",
        "\n",
        "![crossval](https://drive.google.com/uc?id=1i6IhODxbB4bX8O8MmmEpZ9EUsDCPUrI_)\n",
        "\n",
        "### Manual Implementation of k-fold Cross-Validation\n",
        "\n",
        "Let's see how we could manually implement k-fold cross-validation to evaluate a single model (e.g., a linear regression with the best polynomial degree we found earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "manual-cv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# We use the full training data (X_train_val) for cross-validation\n",
        "X_train_val = X_train_df.copy()\n",
        "\n",
        "# Number of partitions/folds to divide the dataset into\n",
        "k_folds = 8\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# This list will store the mean squared error for each fold.\n",
        "val_mses = []\n",
        "\n",
        "# Create the polynomial features for the best degree\n",
        "poly = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
        "numerical_cols = scaled_train_df.columns\n",
        "X_train_val_poly = poly.fit_transform(X_train_val[numerical_cols])\n",
        "poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
        "X_train_val_poly_df = pd.DataFrame(X_train_val_poly, columns=poly_feature_names, index=X_train_val.index)\n",
        "\n",
        "# Combine with one-hot encoded features\n",
        "ohe_cols = ohe_train_df.columns\n",
        "X_train_val_full = pd.concat([X_train_val_poly_df, X_train_val[ohe_cols]], axis=1)\n",
        "\n",
        "\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(X_train_val_full):\n",
        "    # YOUR CODE HERE\n",
        "    # 1. Get the training and validation sets for this fold\n",
        "    # 2. Initialize and train a linear regression model\n",
        "    # 3. Predict on the validation fold and compute MSE\n",
        "    # 4. Append the MSE to the val_mses list\n",
        "    pass\n",
        "\n",
        "# Calculate the average mean squared error across all folds.\n",
        "avg_mse = np.mean(val_mses)\n",
        "\n",
        "# Print the result\n",
        "print(f\"MSE for each fold: {np.round(val_mses, 2)}\")\n",
        "print(f\"Average MSE from manual k-fold CV: {avg_mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j43eOHiDyZlP"
      },
      "source": [
        "> **Question:** When we manually implemented cross-validation with `KFold`, we set `shuffle=True`. Why is shuffling the data before splitting it into folds often important?\n",
        "\n",
        "### K-fold Cross-Validation with scikit-learn\n",
        "\n",
        "Manually implementing cross-validation is great for understanding, but `scikit-learn` provides much easier ways to do it. The `cross_val_score` function is perfect for this. Let's use it to find the best polynomial degree again, but this time with the robustness of cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sklearn-cv-tuning"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "mean_cv_scores = []\n",
        "std_cv_scores = []\n",
        "\n",
        "# We use the full training data (X_train_val) for cross-validation\n",
        "X_train_val = X_train_df.copy()\n",
        "\n",
        "for degree in degrees:\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    numerical_cols = scaled_train_df.columns\n",
        "    X_train_val_poly = poly.fit_transform(X_train_val[numerical_cols])\n",
        "    poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
        "    X_train_val_poly_df = pd.DataFrame(X_train_val_poly, columns=poly_feature_names, index=X_train_val.index)\n",
        "\n",
        "    # Combine with one-hot encoded features\n",
        "    ohe_cols = ohe_train_df.columns\n",
        "    X_train_val_full = pd.concat([X_train_val_poly_df, X_train_val[ohe_cols]], axis=1)\n",
        "\n",
        "    # Initialize a linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Perform k-fold cross-validation\n",
        "    # We use 'neg_mean_squared_error' because scoring functions in sklearn are maximized.\n",
        "    # By maximizing negative MSE, we are minimizing MSE.\n",
        "    cv_scores = cross_val_score(model, X_train_val_full, y_train_val, cv=k_folds, scoring='neg_mean_squared_error')\n",
        "\n",
        "    # Store the mean of the negative MSE scores (and make it positive)\n",
        "    mean_cv_scores.append(-np.mean(cv_scores))\n",
        "    std_cv_scores.append(np.std(cv_scores))\n",
        "\n",
        "# Find the best degree from cross-validation\n",
        "best_degree_cv = degrees[np.argmin(mean_cv_scores)]\n",
        "\n",
        "print(f\"Mean CV MSEs for each degree: {np.round(mean_cv_scores, 2)}\")\n",
        "print(f\"Best polynomial degree from CV: {best_degree_cv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-cv-tuning"
      },
      "outputs": [],
      "source": [
        "mean_cv_scores = np.array(mean_cv_scores)\n",
        "std_cv_scores = np.array(std_cv_scores)\n",
        "\n",
        "plt.plot(degrees, mean_cv_scores, marker='o', label=\"Mean CV MSE\")\n",
        "plt.fill_between(degrees, np.maximum(0, mean_cv_scores - std_cv_scores), mean_cv_scores + std_cv_scores, alpha=0.2, label=\"+/- 1 std. dev.\")\n",
        "plt.xlabel(\"Polynomial Degree\")\n",
        "plt.ylabel(\"Mean Cross-Validated MSE\")\n",
        "plt.title(\"CV MSE vs. Polynomial Degree\")\n",
        "plt.yscale('log')\n",
        "plt.xticks(degrees)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CHZio1nyZlP"
      },
      "source": [
        "> **Question:** In the plot of \"CV MSE vs. Polynomial Degree,\" we used a logarithmic scale for the y-axis (`plt.yscale('log')`). What is the advantage of doing this? What features of the plot does it make easier to see?\n",
        "\n",
        "Notice that with a more robust evaluation method, we've chosen a different \"best\" degree. This highlights the value of cross-validation over a single validation split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7mcnIItyZlP"
      },
      "source": [
        "### Using `cross_validate` for More Detailed Evaluation\n",
        "\n",
        "The `cross_val_score` function is a convenient way to get a single performance metric. However, if you need more detailed information, such as training scores or multiple metrics at once, `cross_validate` is the tool to use. This can be very helpful for diagnosing issues like overfitting, by comparing training and validation performance.\n",
        "\n",
        "Let's use it to evaluate our model with the `best_degree_cv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSjB3sUSyZlP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Create polynomial features with the best degree\n",
        "poly = PolynomialFeatures(degree=best_degree_cv, include_bias=False)\n",
        "numerical_cols = scaled_train_df.columns\n",
        "X_train_val_poly = poly.fit_transform(X_train_val[numerical_cols])\n",
        "poly_feature_names = poly.get_feature_names_out(numerical_cols)\n",
        "X_train_val_poly_df = pd.DataFrame(X_train_val_poly, columns=poly_feature_names, index=X_train_val.index)\n",
        "ohe_cols = ohe_train_df.columns\n",
        "X_train_val_full = pd.concat([X_train_val_poly_df, X_train_val[ohe_cols]], axis=1)\n",
        "\n",
        "# Initialize a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define the metrics we want to compute\n",
        "scoring = ['neg_mean_squared_error', 'r2']\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_results = cross_validate(model, X_train_val_full, y_train_val, cv=k_folds, scoring=scoring, return_train_score=True)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average Val MSE:\", -np.mean(cv_results['test_neg_mean_squared_error']))\n",
        "print(\"Average Val R^2:\", np.mean(cv_results['test_r2']))\n",
        "print(\"Average Train R^2:\", np.mean(cv_results['train_r2']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARh0yd-MyZlP"
      },
      "source": [
        "> **Gotcha!** The `cross_validate` function returns a dictionary where keys like `'test_neg_mean_squared_error'` and `'test_r2'` can be misleading. In this context, \"test\" refers to the score on the **validation fold** for each split, *not* the final, held-out test set that we created at the very beginning. It's a slightly confusing naming choice, but a very important distinction!\n",
        "\n",
        "> **Question:** We used cross-validation on our training data (`X_train_val`) to find the best hyperparameter (the polynomial degree). Now that we have our \"best\" degree, why do we want to re-train a new model on the *entire* `X_train_val` dataset before evaluating it on the test set?\n",
        ">\n",
        "> **Answer Hint:** Think about what we are \"throwing away\" during each fold of cross-validation. We want to use as much data as possible for our final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final-model-evaluation"
      },
      "source": [
        "### Final Model Evaluation\n",
        "\n",
        "Now that we have found the best hyperparameter using cross-validation, we can train our final model on the *entire* training dataset (`X_train_val`) and evaluate its performance on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1k9V_8MyZlP"
      },
      "outputs": [],
      "source": [
        "# Create polynomial features with the best degree found by CV\n",
        "poly_final = PolynomialFeatures(degree=best_degree_cv, include_bias=False)\n",
        "\n",
        "# We use the full training data (X_train_val) and test data (X_test_df)\n",
        "X_train_val = X_train_df.copy()\n",
        "X_test = X_test_df.copy()\n",
        "\n",
        "# Transform numerical features for both train and test sets\n",
        "numerical_cols = scaled_train_df.columns\n",
        "X_train_val_poly_final = poly_final.fit_transform(X_train_val[numerical_cols])\n",
        "X_test_poly_final = poly_final.transform(X_test[numerical_cols])\n",
        "\n",
        "# Get feature names\n",
        "poly_feature_names_final = poly_final.get_feature_names_out(numerical_cols)\n",
        "\n",
        "# Create DataFrames\n",
        "X_train_val_poly_final_df = pd.DataFrame(X_train_val_poly_final, columns=poly_feature_names_final, index=X_train_val.index)\n",
        "X_test_poly_final_df = pd.DataFrame(X_test_poly_final, columns=poly_feature_names_final, index=X_test.index)\n",
        "\n",
        "# Combine with one-hot encoded features\n",
        "ohe_cols = ohe_train_df.columns\n",
        "X_train_val_final = pd.concat([X_train_val_poly_final_df, X_train_val[ohe_cols]], axis=1)\n",
        "X_test_final = pd.concat([X_test_poly_final_df, X_test[ohe_cols]], axis=1)\n",
        "\n",
        "# Fit the final linear regression model\n",
        "final_model = LinearRegression()\n",
        "# Fit the model on the entire training data\n",
        "...\n",
        "# Evaluate on the train set\n",
        "r2_train_final = ...\n",
        "# Evaluate on the test set\n",
        "r2_test_final = ...\n",
        "print(f\"Final R^2 with polynomial degree {best_degree_cv}\")\n",
        "print(f\"Test: {r2_train_final:.3f}\")\n",
        "print(f\"Test: {r2_test_final:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regularization-section-start"
      },
      "source": [
        "## 5 - Improving the Model with Feature Interactions and Regularization\n",
        "\n",
        "### Adding Specific Interaction Terms\n",
        "\n",
        "Our baseline polynomial model is a good start, but we might be able to improve it by adding interaction terms. Instead of adding all possible interactions (which would create too many features), we can select a few key predictors and generate their pairwise interactions. Let's select five features we believe are most importantâ`age`, `page_views`, `fpl_points`, `big_six_1`, and `new_signing_1`âand use `PolynomialFeatures` to cleanly generate all of their pairwise interaction terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-overfit-model"
      },
      "outputs": [],
      "source": [
        "# We use the full training data (X_train_val)\n",
        "X_train_val = X_train_df.copy()\n",
        "X_test = X_test_df.copy()\n",
        "\n",
        "# Start with the feature set from our best polynomial model\n",
        "X_train_complex = X_train_val_final.copy()\n",
        "X_test_complex = X_test_final.copy()\n",
        "\n",
        "# --- Create interaction terms ---\n",
        "# Define the subset of features for which to create interactions\n",
        "features_to_interact = ['age', 'page_views', 'big_six_1']\n",
        "X_train_subset = X_train_val[features_to_interact]\n",
        "X_test_subset = X_test[features_to_interact]\n",
        "\n",
        "# Use PolynomialFeatures to generate pairwise interactions\n",
        "interaction_transformer = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "\n",
        "# Fit on the training data subset and transform both train and test\n",
        "train_interactions = interaction_transformer.fit_transform(X_train_subset)\n",
        "test_interactions = interaction_transformer.transform(X_test_subset)\n",
        "\n",
        "# Get the names of the new interaction features\n",
        "interaction_names = interaction_transformer.get_feature_names_out(features_to_interact)\n",
        "\n",
        "# Create DataFrames with the new features\n",
        "train_interactions_df = pd.DataFrame(train_interactions, columns=interaction_names, index=X_train_val.index)\n",
        "test_interactions_df = pd.DataFrame(test_interactions, columns=interaction_names, index=X_test.index)\n",
        "\n",
        "# The output includes the original columns, so we select only the new interaction terms\n",
        "# Interaction terms are those with a space in their name\n",
        "interaction_only_names = [name for name in interaction_names if ' ' in name]\n",
        "train_interaction_features = train_interactions_df[interaction_only_names]\n",
        "test_interaction_features = test_interactions_df[interaction_only_names]\n",
        "\n",
        "# Add the new interaction features to our complex model's feature set\n",
        "X_train_complex = pd.concat([X_train_complex, train_interaction_features], axis=1)\n",
        "X_test_complex = pd.concat([X_test_complex, test_interaction_features], axis=1)\n",
        "\n",
        "\n",
        "print(f\"Number of features in baseline model: {X_train_val_final.shape[1]}\")\n",
        "print(f\"Number of features in complex model: {X_train_complex.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-qbJh5-yZlP"
      },
      "source": [
        "Our new feature set is massive and the features have a very wide range of scales. Before feeding them into a regularized model, we must scale them. This ensures that the regularization penalty is applied fairly to all features, regardless of their original units or magnitude. Without scaling, features with larger scales would be penalized more heavily, which is not what we want.\n",
        "\n",
        "> **Question:** We are now scaling our entire feature set, which includes binary (0/1) features from one-hot encoding and their interactions. Does it make sense to 'scale' a feature that is already on a 0-1 scale? Why is it still a good practice in this context, especially before regularization?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n22THju-yZlP"
      },
      "outputs": [],
      "source": [
        "# Scale the high-dimensional feature set\n",
        "overfit_scaler = StandardScaler()\n",
        "X_train_complex_scaled = overfit_scaler.fit_transform(X_train_complex)\n",
        "X_test_complex_scaled = overfit_scaler.transform(X_test_complex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split-cell-md-2"
      },
      "source": [
        "Now let's fit a simple linear regression model to this more complex feature set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split-cell-code-2"
      },
      "outputs": [],
      "source": [
        "# Fit a simple linear regression model\n",
        "complex_model = LinearRegression()\n",
        "complex_model.fit(X_train_complex_scaled, y_train_val)\n",
        "\n",
        "# Check the scores\n",
        "r2_train_complex = complex_model.score(X_train_complex_scaled, y_train_val)\n",
        "r2_test_complex = complex_model.score(X_test_complex_scaled, y_test)\n",
        "\n",
        "print(f\"R^2 train (complex model) = {r2_train_complex:.3}\")\n",
        "print(f\"R^2 test (complex model) = {r2_test_complex:.3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rVy47_KyZlP"
      },
      "source": [
        "The added complexity has slightly increased the training R^2 but has hurt the test R^2. The gap between the train and test performance suggests our model is overfitting. This is a perfect scenario to apply regularization, which can help by penalizing large coefficients and simplifying the model.\n",
        "\n",
        "> **Question:** $R^2$ scores can sometimes be negative! What is the interpretation of a negative $R^2$?\n",
        "\n",
        "### Applying Regularization\n",
        "\n",
        "**Regularization** is a technique used to combat overfitting by adding a penalty term to the loss function. This penalty discourages the model from learning overly complex patterns by shrinking the coefficient estimates towards zero.\n",
        "\n",
        "#### Ridge Regression (L2 Penalty)\n",
        "\n",
        "Ridge regression adds a penalty proportional to the square of the magnitude of the coefficients.\n",
        "\n",
        "$$L_{\\text{Ridge}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|y_i - \\beta^\\top x_i \\right|^2 + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$\n",
        "\n",
        "The tuning parameter $\\lambda$ (or `alpha` in scikit-learn) controls the strength of the penalty.\n",
        "\n",
        "#### Lasso Regression (L1 Penalty)\n",
        "\n",
        "Lasso regression adds a penalty proportional to the absolute value of the magnitude of the coefficients.\n",
        "\n",
        "$$L_{\\text{Lasso}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|y_i - \\beta^\\top x_i \\right|^2 + \\lambda \\sum_{j=1}^{p}|\\beta_j|$$\n",
        "\n",
        "A key feature of Lasso is that it can shrink some coefficients to be exactly zero, effectively performing feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dci7xOzMyZlP"
      },
      "source": [
        "> **Question:** Look at the penalty terms for Ridge and Lasso. Why is it crucial that our features are on a similar scale before applying regularization? What would happen if one feature had a much larger scale than the others?\n",
        "\n",
        "> **Question:** Lasso is known for performing feature selection by forcing some coefficients to become exactly zero. Ridge, on the other hand, only shrinks them towards zero. In what practical scenario might you prefer Lasso over Ridge? When might Ridge be the better choice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNzO0C6JyZlP"
      },
      "source": [
        "### Automated Hyperparameter Tuning with Cross-Validation\n",
        "\n",
        "We can use `RidgeCV` and `LassoCV` to automatically find the best `alpha` hyperparameter using cross-validation and fit a regularized model to our overfit dataset.\n",
        "\n",
        "> **Question:** When we set up `RidgeCV` and `LassoCV`, we provided a list of `alphas` created with `np.logspace`. Why is it common practice to search for hyperparameters like the regularization strength on a logarithmic scale rather than a linear one?\n",
        "\n",
        "> **Gotcha!** The `RidgeCV` and `LassoCV` models will sort the list of alphas you provide them. This means that the `mse_path_` attribute on a fitted `LassoCV` model, which contains the MSE for each alpha, will be ordered according to the *sorted* alphas, not necessarily the order you provided them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRsGfS8noxqU",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Set parameters for cross-validation\n",
        "ridge_alphas = np.logspace(-2, 3, 100)\n",
        "lasso_alphas = np.logspace(-3, 0, 100)\n",
        "k = 8\n",
        "\n",
        "# These CV models refit on the full training data\n",
        "# using the best alpha found during cross-validation!\n",
        "ridge = RidgeCV(alphas=ridge_alphas,\n",
        "                cv=k).fit(X_train_complex_scaled, y_train_val)\n",
        "lasso = LassoCV(alphas=lasso_alphas,\n",
        "                cv=k,\n",
        "                max_iter=10000,\n",
        "                tol=0.0001).fit(X_train_complex_scaled, y_train_val)\n",
        "\n",
        "# Best alphas & test scores\n",
        "print(\"[Test R^2]\")\n",
        "# Get the best alpha from the fitted RidgeCV model\n",
        "ridge_a = ...\n",
        "# Print the score on the test set\n",
        "print(f'Ridge /w alpha={ridge_a:.5f}: {...:.3}')\n",
        "\n",
        "lasso_a = lasso.alpha_\n",
        "print(f'LASSO /w alpha={lasso_a:.5f}: {lasso.score(X_test_complex_scaled, y_test):.3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leykFc2UyZlQ"
      },
      "source": [
        "> **Question:** You may sometimes see a `ConvergenceWarning` when running the Lasso model. What does this warning mean? If you see it, does it mean your model is unusable? What parameters could you adjust to try to resolve it? RidgeCV doesn't have these parameters. Why not?\n",
        "\n",
        "> **Gotcha!** The `LassoCV` and `RidgeCV` objects are convenient because they not only find the best `alpha` but also automatically re-fit a final model on the *entire* training dataset using that `alpha`. Why is this final re-fitting step, which `cross_val_score` does *not* do, so important for building the best possible model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAAuuy93yZlQ"
      },
      "outputs": [],
      "source": [
        "n_coefs = len(lasso.coef_)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(n_coefs), lasso.coef_)\n",
        "plt.xlabel(f\"Coefficient Index (Total {n_coefs})\")\n",
        "plt.ylabel(\"Coefficient Value\")\n",
        "plt.title(\"Lasso Coefficients - Many are Zero\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total number of features: {n_coefs}\")\n",
        "print(f\"Number of features with non-zero coefficients: {np.sum(lasso.coef_ != 0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfdVTiT5yZlQ"
      },
      "source": [
        "> **Question:** Now that our features have been scaled, how does the interpretation of a regression coefficient ($\\beta_j$) change?\n",
        ">\n",
        "> **Question:** The coefficients we see are for the scaled features. If we needed to report the model in terms of the original feature units (e.g., for every 1-year increase in age, market value changes by X), how could we use our fitted `StandardScaler` object (`overfit_scaler`) to recover the original-scale coefficients?\n",
        "\n",
        "### Visualizing Regularization Performance\n",
        "\n",
        "Let's visualize how the cross-validated mean squared error changes as we vary the regularization parameter `alpha`. This helps us understand how sensitive the model is to the choice of `alpha` and confirms that our `RidgeCV` and `LassoCV` picked a reasonable value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpRqNwRkyZlQ"
      },
      "outputs": [],
      "source": [
        "# For Lasso, the mse_path_ attribute stores the MSE for each alpha and fold\n",
        "lasso_mses = np.mean(lasso.mse_path_, axis=1)\n",
        "lasso_stds = np.std(lasso.mse_path_, axis=1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(lasso.alphas_, lasso_mses, 'b-')\n",
        "plt.fill_between(lasso.alphas_, np.maximum(0, lasso_mses - lasso_stds), lasso_mses + lasso_stds, color='b', alpha=0.2, label=\"+/- 1 std. dev.\")\n",
        "plt.axvline(lasso.alpha_, color='r', linestyle='--', label=f'Best alpha = {lasso.alpha_:.3f}')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.gca().invert_xaxis() # Alphas in LassoCV are in descending order\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Lasso CV Performance')\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNp5cBDcyZlQ"
      },
      "source": [
        "### Coefficient Path (Trace Plots)\n",
        "\n",
        "A great way to visualize the effect of regularization is to plot the value of each coefficient as a function of the regularization parameter `alpha`. This is often called a \"trace plot\" or \"coefficient path\".\n",
        "\n",
        "- For **Ridge**, we expect coefficients to shrink smoothly towards zero as `alpha` increases.\n",
        "- For **Lasso**, we expect coefficients to shrink as well, but some will be forced to *exactly* zero, effectively removing them from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_v_hn3uyZlQ"
      },
      "outputs": [],
      "source": [
        "# We'll use the same overfitted data\n",
        "X = X_train_complex_scaled\n",
        "y = y_train_val\n",
        "\n",
        "# Alphas to iterate over\n",
        "alphas = np.logspace(-3, 3, 200)\n",
        "\n",
        "# Store coefficients\n",
        "ridge_coefs = []\n",
        "lasso_coefs = []\n",
        "\n",
        "for a in alphas:\n",
        "    ridge = Ridge(alpha=a, fit_intercept=True)\n",
        "    ridge.fit(X, y)\n",
        "    ridge_coefs.append(ridge.coef_)\n",
        "\n",
        "    lasso = Lasso(alpha=a, fit_intercept=True, max_iter=10000, tol=0.0001)\n",
        "    lasso.fit(X, y)\n",
        "    lasso_coefs.append(lasso.coef_)\n",
        "\n",
        "# Plotting\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
        "\n",
        "# Ridge Trace Plot\n",
        "ax1.plot(alphas, ridge_coefs)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel('alpha')\n",
        "ax1.set_ylabel('Coefficients')\n",
        "ax1.set_title('Ridge Coefficients Path')\n",
        "ax1.grid(True)\n",
        "\n",
        "# Lasso Trace Plot\n",
        "ax2.plot(alphas, lasso_coefs)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_xlabel('alpha')\n",
        "ax2.set_title('Lasso Coefficients Path')\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhpVGJiayZlQ"
      },
      "source": [
        "Success! By applying regularization, we were able to control the complexity of the model with many features, leading to much better performance on the test set. Both Ridge and Lasso significantly improved the R^2 score compared to the simple linear regression model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "default_lexer": "ipython3",
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}