{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJXfvGVDQJ9o"
   },
   "source": [
    "# CSCI E-89 Deep Learning\n",
    "## LAB 02: Review of Calculus, Backpropagation, Logistic Regression\n",
    "\n",
    "References: Lecture Notes \\ Planar data classification with one hidden layer - Andrew Ng \\ https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brO3-VJxByLm"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ox3IMn6eKKsl",
    "outputId": "d557ed5a-86d7-465f-8bad-040a586396d6"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDdxwlSjAQqI"
   },
   "source": [
    "# CALCULUS REVIEW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EASLE1D7Agtk"
   },
   "source": [
    "**Slope**\n",
    "\n",
    "How does a change in $x$ affect $y$\n",
    "\n",
    "$$ slope = \\frac{rise}{run} = \\frac{change \\: in \\: y}{change \\:in \\: x}\n",
    "= \\frac{y_1 - y_0}{x_1- x_0}\n",
    "$$ \n",
    "\n",
    "----\n",
    "\n",
    "**Derivative - definition:** \n",
    "\n",
    "\n",
    "$$ \\frac{d}{dx}f(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "\n",
    "Example: \n",
    "$$f(x) = 3x^2 \\\\\n",
    "f^{'}(x) = \\frac{d}{dx}3x^2  = \\lim_{h \\to 0} \\frac{3(x+h)^2 - 3x^2}{h} \\\\\n",
    "= \\lim_{h \\to 0}\\frac{3x^2+6xh+3h^2-3x^2}{h} \\\\\n",
    "= \\lim_{h \\to 0}\\frac{6xh+3h^2}{h} \\\\\n",
    "= \\lim_{h \\to 0}\\frac{6x\\require{cancel} \\cancel{h}+3\\require{cancel} \\cancelto{h}{h^2}}{\\require{cancel} \\cancel{h}} \\\\\n",
    "= \\lim_{h \\to 0}{6x+3 \\require{cancel} \\cancelto{0}{h}} \\\\\n",
    "= 6x \\\\\n",
    "$$\n",
    "Intuition: When $x$ goes up by 1 unit, $y$ goes by 6 units \n",
    "\n",
    "---\n",
    "\n",
    "**Partial Derivative:**\n",
    "\n",
    "Take derivative with respect to named variable and treat other variables as constants. \n",
    "\n",
    "Example:\n",
    "$$f(x, y,z) = z = x^2y + xy^3 + z$$\n",
    "\n",
    "If we don't change $y$, how does change in $x$, affect $z$. In other words find $\\frac{\\partial f}{\\partial x} $\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x}  = 2xy + y^3 $$\n",
    "\n",
    "If we don't change $x$, how does change in $y$, affect $z$. In other words find $\\frac{\\partial f}{\\partial y} $\n",
    "$$\\frac{\\partial f}{\\partial y}  = x^2 + 3xy^2$$ \n",
    "\n",
    "----\n",
    "\n",
    "**Gradient**\n",
    "$$\\nabla (f)= \\frac{\\partial f}{\\partial x} \\hat{i} + \\frac{\\partial f}{\\partial y} \\hat{j}+ \\frac{\\partial f}{\\partial z} \\hat{k} = \\frac{\\partial f}{\\partial x} \\hat{x} + \\frac{\\partial f}{\\partial y} \\hat{y}+ \\frac{\\partial f}{\\partial z} \\hat{z}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4Yk7LD9cX7w"
   },
   "source": [
    "**Inverse Function**\n",
    "\n",
    "Suppose a function $f$ maps x $\\rightarrow$ y, then inverse $f^{-1}$ maps $y \\rightarrow x $  \n",
    "\n",
    "This means: \n",
    "\n",
    "$$f^{-1}(f)= x$$  \n",
    "\n",
    "\n",
    "Example:\n",
    "$$f(x) = y = x^2 \\\\\n",
    "f^{-1}(y) =y^{1/2} \\\\\n",
    "f^{-1}(f) = (x^2)^{1/2} = x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Derivative of Inverse Function** \n",
    "\n",
    " $$ f' = \\frac{dy}{dx} \\\\\n",
    "(f^{-1})' = \\frac{1}{\\frac{dy}{dx}} = (\\frac{dy}{dx})^{-1}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qg9lqWjdunTz"
   },
   "source": [
    "---\n",
    "**Plot $f(x, y) = z = x^2y + xy^3$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "d8tgQSAYlfdC",
    "outputId": "a8fa6394-5c2c-47fa-89a5-817412bb2abb"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "#from numpy import * # cheat to import all numpy functions without needing to type np. everything, usually no conflicts\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.linspace(-5, 5, 50)\n",
    "y = np.linspace(-5, 5, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (X**2)*Y + X*Y**3\n",
    "\n",
    "figure = plt.figure(1, figsize = (12, 4))\n",
    "subplot3d = plt.subplot(111, projection='3d')\n",
    "surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1) #also can just write cmap = 'coolwarm'\n",
    "plt.title(r'$z = x^2y + xy^3$')  # use r'' to engage latex notation for a string\n",
    "subplot3d.set_xlabel('X axis')\n",
    "subplot3d.set_ylabel('Y axis')\n",
    "subplot3d.set_zlabel('Z axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aWfonQeu5KI"
   },
   "source": [
    "___\n",
    "**Plot $\\frac{\\partial f}{\\partial x}  = 2xy + y^3 $**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "Zf3EEmCRtVjT",
    "outputId": "e01932bb-2613-4e32-b4a9-7ca88940d208"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 50)\n",
    "y = np.linspace(-5, 5, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 2*X*Y + Y**3\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "figure = plt.figure(1, figsize = (12, 4))\n",
    "subplot3d = plt.subplot(111, projection='3d')\n",
    "surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, linewidth=0.1)\n",
    "plt.title(r'$\\frac{df}{dx} = 2xy + y^3$')\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "subplot3d.set_zlabel('Z axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTZbzFDzvK-T"
   },
   "source": [
    "---\n",
    "**Plot $\\frac{\\partial f}{\\partial y}  = x^2 + 3xy^2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "oyyLpYcut5-u",
    "outputId": "f5dd57ca-5ad1-45f9-d561-434cfd0cecf7"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 50)\n",
    "y = np.linspace(-5, 5, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 3*X*Y**2\n",
    "figure = plt.figure(1, figsize = (12, 4))\n",
    "subplot3d = plt.subplot(111, projection='3d')\n",
    "surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1)\n",
    "plt.title(r'$\\frac{df}{dy} = 2xy + y^3$')\n",
    "subplot3d.set_xlabel('X axis')\n",
    "subplot3d.set_ylabel('Y axis')\n",
    "subplot3d.set_zlabel('Z axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Plot $\\frac{\\partial f}{\\partial y}  = x^2y + xy^3$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "vxiXo8x_ryXv",
    "outputId": "6cdef810-d0f7-4e6a-8374-89f3fdb64f40"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 50)\n",
    "y = np.linspace(-5, 5, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (X**2)*Y + X*Y**3\n",
    "\n",
    "figure = plt.figure(1, figsize = (12, 4))\n",
    "subplot3d = plt.subplot(111, projection='3d')\n",
    "surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IK1od_wRuEK"
   },
   "source": [
    "---\n",
    "**Product Rule**\n",
    "\n",
    "$$ (f.g)^{'} = f^{'}g + fg^{'}$$\n",
    "\n",
    "Example: \n",
    "$$f(x) = (4x^2+x)(x^3+8x^2)$$\n",
    "$$f^{'}(x) = (8x+1)(x^3+8x^2) + (4x^2+x)(3x^2+16x)$$\n",
    "\n",
    "---\n",
    "**Chain Rule**\n",
    "\n",
    "$$ F^{'}(x) = f^{'}(g(x)) g^{'}(x)$$\n",
    "\n",
    "If $y = f(u)$ and  $u = g(x)$,  then $\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}$\n",
    "\n",
    "Example: \n",
    "$$f(x) = \\sin(3x^2+x) = \\sin(g(x))$$\n",
    "$$f^{'}(x) = \\cos(g(x))g'(x) = \\cos(3x^2+x)(6x+1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a8cBgHkwI2qZ"
   },
   "source": [
    "---\n",
    "---\n",
    "# Using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For autodifferentiation to properly work we need to calculate the gradients and remember the order they were calculated during forward propagation. For backpropagation we follow the operations in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWMlU7KeI1kr"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_derivative(x):\n",
    "  with tf.GradientTape() as tape:  #GradientTape records operations for automatic differentiation\n",
    "    tape.watch(x)  # Ensures that tensor is being traced by this tape.\n",
    "    y = 3*tf.pow(x, 2)\n",
    "  dy_dx = tape.gradient(y,x)\n",
    "  return dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.GradientTape` function records the relevant operations executed within the context in which the function is called, here the `with` session, onto a `tape`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TAyqdP3eOSla",
    "outputId": "13ff3b4c-91e6-4c3a-e4a5-75f4480e9d90"
   },
   "outputs": [],
   "source": [
    "x = tf.constant(2.0)\n",
    "get_derivative(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see just the value of th `tf.Variable` use `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we do? Defined $y = 3 x^2$ and the derivative as $y' = 6x$, so that $y'(2) = 6*2 = 12$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0JcI6Ts7LBQw",
    "outputId": "c8c2926e-b007-400f-bf02-2af771c39ab4"
   },
   "outputs": [],
   "source": [
    "# We could go through same calculations without \n",
    "# defining a TF function, like:\n",
    "x = tf.Variable(2.0 )\n",
    "with tf.GradientTape() as tape:\n",
    "  # tape.watch(x)\n",
    "  y = 3*tf.pow(x, 2) \n",
    "dy_dx = tape.gradient(y, x) \n",
    "\n",
    "print( dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. For individual operations and test like we are doing here, the memory consumed by a persistent tape is negligible. However, for many calculations persisting the tape can have an effect if memory is not flushed and we shoudl do it with care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "# with tf.GradientTape() as tape:\n",
    "with tf.GradientTape(persistent=True) as tape:  # the purpose of persistence to call the gradient multiple times\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "                                                                                                                                                                                                                                                                                                                                                \n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
    "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "For example the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: TF formatting has become much simpler in recent years, so you can use standard pythonic formating as opposed to explicit functions for each operation, e.g., `x**2` instead of `tf.pow(x,2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "exM_DfIfMiMI",
    "outputId": "b5a40cd9-9726-4a77-82f2-a06faa26f17d"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x) \n",
    "    y = 3*x**2 \n",
    "  dy_dx = tape.gradient(y, x) \n",
    "  assert dy_dx is not None\n",
    "  print('dx_dy is:', dy_dx.numpy())\n",
    "except:\n",
    "  print('dx_dy is:', dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjmMRyVeazD6"
   },
   "source": [
    "**Example with constant not define**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hcysQa9zLg05",
    "outputId": "f721b93b-4193-48ac-8172-9a033ead3116"
   },
   "outputs": [],
   "source": [
    "x = tf.constant (2.0)\n",
    "try:\n",
    "  with tf.GradientTape() as tape:\n",
    "#     tape.watch(x) # toggle the comment in and out to see the effect\n",
    "    y = 3*x**2 \n",
    "  dy_dx = tape.gradient(y, x) \n",
    "  assert dy_dx is None\n",
    "  print('dx_dy is', dy_dx)\n",
    "except:\n",
    "    pass\n",
    "    print('dx_dy is', dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qswtu33aiVj"
   },
   "source": [
    "**Second order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GocZRWAbarfI"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_second_derivative(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    with tf.GradientTape() as t:\n",
    "      t.watch(x)\n",
    "      y = x**2 + x\n",
    "    dx_dy = t.gradient(y,x)\n",
    "  d2x_dy = tape.gradient(dx_dy, x)\n",
    "  return dx_dy, d2x_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rrIdZJqYasaU",
    "outputId": "e112aa24-64e0-4586-8750-cef4632e6509"
   },
   "outputs": [],
   "source": [
    "x = tf.constant (3.0)\n",
    "dx_dy, d2x_dy = get_second_derivative(x)\n",
    "print(dx_dy)\n",
    "print(d2x_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we do? Define $y = x^2 + x$, differentiation then calculates the derivateives. Note that TF crowd cal all derivates the gradients. \n",
    "\n",
    "$y' = 2x + 1$ and $y'' = 2$\n",
    "\n",
    "Insert the value $x=2$ to get $y'(2) = 2\\cdot2+1=5$ and $y''(2) = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lT0pdz3r7PTJ"
   },
   "outputs": [],
   "source": [
    "# persistent\n",
    "x = tf.Variable (2.0 )\n",
    "y = tf.Variable (2.0 )\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  z = x**2 + y*x\n",
    "dz_dx = tape.gradient(z, x)\n",
    "dz_dy = tape.gradient(z, y)\n",
    "del tape\n",
    "print(dz_dx)\n",
    "print(dz_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bX04ZCZ37kkD",
    "outputId": "ebf422da-bc44-4872-8a83-7c3cb8e0e648"
   },
   "outputs": [],
   "source": [
    "dz_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bCRWdGPR8Klh",
    "outputId": "dd6335c6-3337-47bc-9c1b-efeebc3d87f8"
   },
   "outputs": [],
   "source": [
    "dz_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6uILdSn2pdr"
   },
   "source": [
    "# Computation Graphs\n",
    "\n",
    "![Computation Graphs](https://drive.google.com/uc?export=view&id=1wXSeGQ_oFJ5awfI8TSAQUQldMY4WTdD_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Equation 1:** $z = xy+ 1/x = a + b$, given $x =1, y = 2$ \n",
    "\n",
    "![title](eq1.png)\n",
    "\n",
    "\n",
    "**Compute forward and cache derivatives simultaneously at each node**\n",
    "\n",
    "for $a$\n",
    "\n",
    "$$a\\bigg|_{x=1, y=2} = xy\\bigg|_{x=1, y =2} = 2 \\\\\n",
    "\\frac{\\partial a}{\\partial y}  = x =1\\: ,\\ \\ \\ \\ \\frac{\\partial a}{\\partial x}  = y =2$$ \n",
    "\n",
    "for $b$\n",
    "$$b = \\frac{1}{x} = 1 \\\\ \n",
    "\\frac{\\partial b}{\\partial x}\\bigg|_{x=1}  = -\\frac{1}{x^2}\\bigg|_{x=1} = -1$$ \n",
    "\n",
    "for $z$\n",
    "$$z = a + b $$\n",
    "\n",
    "$$ z \\bigg|_{a=2, b =1} = 2 + 1 = 3$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial a} = 1, \\: \\: \\frac{\\partial z}{\\partial b} = 1$$ \n",
    "\n",
    "Finally: \n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial a} * \\frac{\\partial a}{\\partial x} + \\frac{\\partial z}{\\partial b} * \\frac{\\partial b}{\\partial x} =\n",
    "(1*2) + (1*-1) =1$$ \n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial a} * \\frac{\\partial a}{\\partial y}  =\n",
    "(1*1) =1$$\n",
    "\n",
    "---\n",
    "\n",
    "**Equation 2:** $z =  \\frac{x+y}{x^2-4}$, given $x =1, y = 2$ \n",
    "\n",
    "![title](eq2.png)\n",
    "\n",
    "**Compute forward and cache derivatives simultaneously at each node**\n",
    "\n",
    "for $a$\n",
    "\n",
    "$$a\\bigg|_{x=1, y=2} = x + y\\bigg|_{x=1, y =2} = 3 \\\\\n",
    "\\frac{\\partial a}{\\partial y}  =1\\: , \\frac{\\partial a}{\\partial x}  = 1$$ \n",
    "\n",
    "for $b$\n",
    "\n",
    "$$b\\bigg|_{x=1} = (x^2-4)\\bigg|_{x=1} = -3 \\\\\n",
    "\\frac{\\partial b}{\\partial x}\\bigg|_{x=1} = 2x\\bigg|_{x=1} =2$$ \n",
    "\n",
    "for $z$\n",
    "\n",
    "$$z = \\frac{a}{b}$$\n",
    "$$ z \\bigg|_{a=3, b =-3} = \\frac{3}{-3} = -1$$ \n",
    "$$\\frac{\\partial z}{\\partial a} = \\frac{1}{b} = -\\frac{1}{3}; \\: \\: \\frac{\\partial z}{\\partial b} = \\frac{-a}{b^2} = -\\frac{1}{3}$$\n",
    "\n",
    "Finally: \n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial a} * \\frac{\\partial a}{\\partial x} + \\frac{\\partial z}{\\partial b} * \\frac{\\partial b}{\\partial x} =\n",
    "(-\\frac{1}{3} *1) + (-\\frac{1}{3}*2) =-1$$ \n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial a} * \\frac{\\partial a}{\\partial y}  =\n",
    "(-\\frac{1}{3}*1) =-\\frac{1}{3}$$ \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Equation 3:** $z =  \\frac{2x+\\frac{1}{x}}{x+y}$, given $x =1, y = 2$ \n",
    "\n",
    "![title](eq3.png)\n",
    "\n",
    "**Compute forward and cache derivatives simultaneously at each node**\n",
    "\n",
    "for $a$\n",
    "\n",
    "$$a\\bigg|_{x=1, y=2} = 2x \\bigg|_{x=1, y =2} = 2 \\\\\n",
    "\\frac{\\partial a}{\\partial y}  =0\\: , \\frac{\\partial a}{\\partial x}  = 2$$  \n",
    "\n",
    "for $b$\n",
    "\n",
    "$$b\\bigg|_{x=1} = \\frac{1}{x}\\bigg|_{x=1} = 1 \\\\\n",
    "\\frac{\\partial b}{\\partial x}\\bigg|_{x=1} = -\\frac{1}{x^2}\\bigg|_{x=1} =-1$$  \n",
    "\n",
    "for $c$\n",
    "\n",
    "$$c = x + y$$\n",
    "$$ c \\bigg|_{x=1, y =2} = 1 + 2 = 3$$ \n",
    "$$\\frac{\\partial c}{\\partial x} = 1 ; \\: \\: \\frac{\\partial c}{\\partial y} = 1$$\n",
    "\n",
    "\n",
    "for $d$\n",
    "\n",
    "$$d = a + b$$\n",
    "$$ d \\bigg|_{a=2, b =1} = 2+ 1 = 3$$ \n",
    "$$\\frac{\\partial d}{\\partial a} = 1 ; \\: \\: \\frac{\\partial d}{\\partial b} = 1$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial d}{\\partial x} = \\frac{\\partial d}{\\partial a} * \\frac{\\partial a}{\\partial x} +  \\frac{\\partial d}{\\partial b} * \\frac{\\partial b}{\\partial x} =(1*2) + (1*-1) = 1$$\n",
    "\n",
    "for $z$\n",
    "\n",
    "$$z = \\frac{d}{c}$$\n",
    "$$ z \\bigg|_{c=3, d =3} = \\frac{3}{3} = 1$$\n",
    "$$\\frac{\\partial z}{\\partial d} = \\frac{1}{c} = \\frac{1}{3}; \\: \\: \\frac{\\partial z}{\\partial c} = \\frac{-d}{c^2} = -1$$\n",
    "\n",
    "Finally: \n",
    "\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial d} * \\frac{\\partial d}{\\partial x} + \\frac{\\partial z}{\\partial c} * \\frac{\\partial c}{\\partial x} =(-1 *1) + (-1 *1) =-2$$ \n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial c} * \\frac{\\partial c}{\\partial y}  =\n",
    "(-1*1) =-1$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lt6uJhYlakwa"
   },
   "source": [
    "---\n",
    "# LOGISTIC REGRESSION\n",
    "In this section we will use neural networks to solve a Logistic Regression Problem. We will use standard python libraries to implement the neural network from scratch\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQ8N8hFLag3W"
   },
   "source": [
    "Logistical Regression solves the following problem: given an input $X \\in \\mathbb{R}^{m\\times n}$ where $m$ is the number of samples and $n$ is the size of input, find $\\hat{y}$ in $Y \\in \\mathbb{R}^{1\\times m}$ such that probability of y=1, given x:\n",
    "$$\\hat y = P(y =1 | x)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm7NeWLyVcfX"
   },
   "source": [
    "$$ X \\in \\mathbb{R}^{m\\times n_{x}} =\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & x_1^{(4)} & \\cdots & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & x_2^{(4)} & \\cdots & x_2^{(m)}\\\\\n",
    "x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & x_3^{(4)} & \\cdots & x_3^{(m)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & x_n^{(4)} & \\cdots & x_n^{(m)}\\\\\n",
    "\\end{bmatrix}}_{\\displaystyle m}\n",
    "\\left.\\vphantom{\\begin{bmatrix}\n",
    "x_1^{(1)} & x_1^{(2)} & x_1^{(3)} & x_1^{(4)} & \\cdots & x_1^{(m)}\\\\\n",
    "x_2^{(1)} & x_2^{(2)} & x_2^{(3)} & x_2^{(4)} & \\cdots & x_2^{(m)}\\\\\n",
    "x_3^{(1)} & x_3^{(2)} & x_3^{(3)} & x_3^{(4)} & \\cdots & x_3^{(m)}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "x_n^{(1)} & x_n^{(2)} & x_n^{(3)} & x_n^{(4)} & \\cdots & x_n^{(m)}\\\\\n",
    "\\end{bmatrix}}\\right\\}n_x$$\n",
    "\n",
    "\n",
    "$$ Y \\in \\mathbb{R}^{1\\times m} =\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & y^{(3)} & y^{(4)} & \\cdots & y^{(m)}\\\\\n",
    "\\end{bmatrix}}_{\\displaystyle m}\n",
    "\\left.\\vphantom{\\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & y^{(3)} & y^{(4)} & \\cdots & y^{(m)}\\\\\n",
    "\\end{bmatrix}}\\right\\}1$$\n",
    "\n",
    "$$w \\in \\mathbb{R}^{n_{x}} $$\n",
    "\n",
    "\n",
    "$$b \\in \\mathbb{R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHtWb03E5Chq"
   },
   "source": [
    "---\n",
    "## Loss Function: Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALAyPOfP3bZ5"
   },
   "source": [
    "![Linera Regression](https://drive.google.com/uc?export=view&id=1j4kRMNeNqe76Xtva-Sar_O9uaX9siDTh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex1gl0lFe1-r"
   },
   "source": [
    "---\n",
    "## Binary Classification\n",
    "\n",
    "We make the following assumptions:\n",
    "1. There are 2 classes in the dataset. Each sample in the dataset belongs to one class or the other but not to both. For example, in classifying whether an image is a cat or dog, every image is either a cat or dog and not a girafe\n",
    "2. Each class is independent of the other class. For example, classifying an image as a cat, does not affect the classification of the next sample. (independent and identically distributed) [Wikipedia: Independent and identically distributed random variables](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "3. All samples are generated from the same distribution. For example, if we train the network with images of cats and dogs, then all images should images of cats and dogs and not others from hotels and bridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5elm3hFZoa3b"
   },
   "source": [
    "---\n",
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1Wrq8vKopIX"
   },
   "source": [
    "$$z=w_1x_1+w_2x_2+b \\\\\n",
    "a = \\hat y = \\sigma(z) \\:\\:  where \\:\\:  \\sigma = \\frac{1}{1+e^{-z}} $$ \n",
    "\n",
    "\n",
    "$a$ is the range of $0$ to 1. If prob > 0.5, assign to 1 else assign to 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NqA6NiIxm3G"
   },
   "source": [
    "---\n",
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfUm2dvBc9fR"
   },
   "source": [
    "With these assumptions in place, the loss of a single training sample can be calculated as follows: \n",
    "\n",
    "$$\\text{Binary Cross Entropy Loss} = -y_i\\log(\\hat y_i)-(1-y_i)\\log(1-\\hat y_i))$$ \n",
    "\n",
    "where $i$ is the $i^{th}$ training sample and  $\\hat y = P(y =1 | x)$, probability that y =1 , given x.\n",
    "\n",
    "**The total loss for all training samples** \n",
    "\n",
    "$$J(w)=L(\\hat y, y)= -\\frac{1}{m}\\sum_{i=1}^m{(y_i\\log(\\hat y_i)+(1-y_i)\\log(1-\\hat y_i))}$$  \n",
    "\n",
    " is the sum of losses of all samples where $m$ is the total number of training samples in the dataset. We also the scale the loss by the number of training samples $m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4maKHcFVM3r0"
   },
   "source": [
    "---\n",
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RC56n0Mmw0_B"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=1ggATcM-i81yEPjWPvXfdAMxNoPSL6M5I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3Zg7NOvNH_U"
   },
   "source": [
    "1. Randomly initialize $w_1, w_2 \\: and \\: b $\n",
    "2. Select learning rate $\\alpha$ \n",
    "\n",
    "3. Update $w_1, w_2 \\: and \\: b $ for all $m$ samples:\n",
    "$$w_1 = w_1 - \\alpha \\frac{\\partial L}{\\partial w_1} = w_1-\\alpha(a-y)x_1$$\n",
    "$$w_2 = w_2 - \\alpha \\frac{\\partial L}{\\partial w_2} = w_2-\\alpha(a-y)x_2$$\n",
    "$$b = b - \\alpha \\frac{\\partial L}{\\partial b} = w_1-\\alpha(a-y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiUFfGgssMQZ"
   },
   "source": [
    "---\n",
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ricwwUusR22"
   },
   "source": [
    "$$\\frac{\\partial L}{\\partial a}  = \\frac{\\partial }{\\partial a}[-y\\log(a)-(1-y)\\log(1-a)] = -y \\frac{1}{a} - (-1)\\frac{1-y}{1-a} = \\frac{-y}{a} + \\frac{1-y}{1-a} \\: $$ \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} * \\frac{\\partial a}{\\partial z} $$ \n",
    "\n",
    "$\\text{Recall} \\: a = \\sigma (z) \\: \\text{and from  Lecture} \\:\\: \\frac{d}{dz}[\\sigma(z)] = \\frac{\\partial a}{\\partial z} = (1-\\sigma(z))\\sigma(z)$ such that $\\frac{\\partial a}{\\partial z}= (1-a)a $\n",
    "\n",
    "Therefore $\\:  \\frac{\\partial L}{\\partial z} = [\\frac{-y}{a} + \\frac{1-y}{1-a}](1-a)a \n",
    "=[\\frac{-y}{a}](1-a)a + [\\frac{1-y}{1-a}](1-a)a \n",
    "= -y(1-a) + (1-y)a \n",
    "= -y +ay+a -ay \\\\\n",
    "=a-y \\:\\: \n",
    "$\n",
    "\n",
    "$$\\frac{dz}{dw_1} = x_1$$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a} * \\frac{\\partial a}{\\partial z} * \\frac{\\partial z}{\\partial w_1} \n",
    "= [\\frac{-y}{a} + \\frac{1-y}{1-a}]*[(1-a)a]*x_1 \n",
    "= [-y(1-a) + a(1-y)]x \n",
    "= [-y +ay + a -ay]x \\\\\n",
    "\\;\\;\\;\\;\\;\\;=(a-y)x \\:\\: [Equation 1]\n",
    "$\n",
    "\n",
    "\n",
    "$$\\frac{dz}{db} = 1$$ \n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial a} * \\frac{\\partial a}{\\partial z} * \\frac{\\partial z}{\\partial b} \n",
    "= [\\frac{-y}{a} + \\frac{1-y}{1-a}]*[(1-a)a]*1 \n",
    "= [-y(1-a) + a(1-y)] \n",
    "= [-y +ay + a -ay] \\\\\n",
    "\\;\\;\\;\\;\\;\\;=(a-y) \\:\\: [Equation 2]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z91rGEFb5Bku"
   },
   "source": [
    "---\n",
    "# Implemention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKxhxtyi5KzM"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=185am24W4f7rgJXosnbpESMsBkjQofBLp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example of a logistic problem we will examine the commonly used flower classification problem. The dataset considers three types of irides (iris) but we will only consider two here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgu10ooCByLv"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gsAA_r8ByL1"
   },
   "outputs": [],
   "source": [
    "X = iris.data[:, :2] # use only 2 features\n",
    "y = (iris.target != 0) * 1 # force to 2 classes\n",
    "# y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6-Y1v0J6ByL5",
    "outputId": "9c960b64-4e4b-402c-bd6d-ded6665aa83f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')\n",
    "# plt.scatter(X[y == 2][:, 0], X[y == 2][:, 1], color='k', label='2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VRDC-ga6lVA1",
    "outputId": "aac68c97-376f-45c7-db59-304195778e0a"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APZYZkJdtbz9"
   },
   "outputs": [],
   "source": [
    "X = X.T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vsXY9NTAk831",
    "outputId": "4b42d781-da14-4c24-9320-ee446427f7c9"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gNLF8QpYtmFf"
   },
   "outputs": [],
   "source": [
    "y = y.reshape(1, 150)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKLJyQuBRjSL"
   },
   "source": [
    "---\n",
    "# Implement Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBGG7fUcRpKi"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "YCcLmACcR3bB",
    "outputId": "71ecbad0-513b-4989-88fe-c780c9f50ca6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x=np.linspace(-20,20,1000)\n",
    "plt.plot(x,sigmoid(x),'b',)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.title('Sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x=np.linspace(-20,20,1000)\n",
    "plt.plot(x,np.tanh(x),'b',)\n",
    "plt.plot([-20,20],[0,0],'k--',linewidth=0.1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "plt.title('Hyperbolic Tangent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5wt4MTka8RU"
   },
   "source": [
    "---\n",
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2iTngs1wukL"
   },
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer   \n",
    "    return (n_x, n_h, n_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SIzVgmnpw0aF",
    "outputId": "c1e73947-9fcc-41a4-f15b-f14abb459b6a"
   },
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = layer_sizes(X, y)\n",
    "print(n_x)\n",
    "print(n_h)\n",
    "print(n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVxsJSnObCJN"
   },
   "outputs": [],
   "source": [
    "def model(x_train, y_train, num_layers):\n",
    "  n_x = x_train.shape[0] # size of x\n",
    "  n_h = num_layers\n",
    "  n_y = y_train.shape[0] # output size\n",
    "  return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Un34Zpw0dqnK",
    "outputId": "83bbf6e7-bfb0-474d-a749-f7de062c16a2"
   },
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = model(x_train = X, y_train = y, num_layers=4)\n",
    "print(n_x)\n",
    "print(n_h)\n",
    "print(n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTt7BLUcSsBK"
   },
   "source": [
    "---\n",
    "# Initialize Weights\n",
    "\n",
    "1. We do not want to initialize the weights, $w$, to all the hiddens units in a symmetric way, otherwise they will compute the same function\n",
    "2. Initialize $w$ to a small but non-zero, random value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EH2yQSqpSXtZ"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "    \n",
    "    W1 = 0.01 * np.random.randn(n_h,n_x)  # random normal distribution\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = 0.01 * np.random.randn(n_y,n_h) \n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x)) # assert gives an error, as opposed to just passing False forward\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEIqaKGagXL8"
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "yrJyPK2Agdvo",
    "outputId": "0abdfea9-6296-48de-acc2-67fe76e53c38"
   },
   "outputs": [],
   "source": [
    "print(parameters['W1'].shape)\n",
    "print(parameters['b1'].shape)\n",
    "print(parameters['W2'].shape)\n",
    "print(parameters['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YgCnZrcS5-u"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "   \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {'Z1': Z1,\n",
    "             'A1': A1,\n",
    "             'Z2': Z2,\n",
    "             'A2': A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe3Rg2XInIHU"
   },
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X = X, parameters = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFGz4ekpnj8Z"
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    \n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)\n",
    "    cost = - np.sum(logprobs) * (1 / m) \n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect,  remove single-dimensional entries from the shape of an array\n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9q9XixCn2KK"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    \n",
    "    dZ2= A2 - Y\n",
    "    dW2 = 1 / m *(np.dot(dZ2,A1.T))\n",
    "    db2 = 1 / m *(np.sum(dZ2,axis = 1,keepdims = True))\n",
    "    dZ1 = np.dot(W2.T,dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1 / m *(np.dot(dZ1,X.T))\n",
    "    db1 = 1 / m *(np.sum(dZ1,axis = 1,keepdims = True))\n",
    "    \n",
    "    \n",
    "    grads = {'dW1': dW1,\n",
    "             'db1': db1,\n",
    "             'dW2': dW2,\n",
    "             'db2': db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "TykMa7WuoJX8",
    "outputId": "470c4b28-ac57-4f71-e10e-0e6d78b43085"
   },
   "outputs": [],
   "source": [
    "grads = backward_propagation(parameters, cache, X, y)\n",
    "print ('dW1 =', (grads[\"dW1\"]))\n",
    "print ('db1 =', (grads[\"db1\"]))\n",
    "print ('dW2 =', (grads[\"dW2\"]))\n",
    "print ('db2 =', (grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TvFn3SnQnoch"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    \n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    \n",
    "    # Update weights\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1 = initialize_parameters(n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "LDi4V_R2v8g-",
    "outputId": "230dff1f-5173-41d3-f8b6-b20b09f69305"
   },
   "outputs": [],
   "source": [
    "parameters2 = update_parameters(parameters1, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters1[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters1[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters1[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters1[\"b2\"])+'\\n')\n",
    "\n",
    "\n",
    "print(\"W1 = \" + str(parameters2[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters2[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters2[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters2[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_46Eug3HRyQ"
   },
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, lr=1.2, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    \n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    history = {}\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X,parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads,learning_rate = lr)\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "eO4wgFL6H5GE",
    "outputId": "d9ae10b8-e3dc-4332-fa76-e2e6cee0e3fa"
   },
   "outputs": [],
   "source": [
    "parameters= model(X, y, 10, 0.3, num_iterations=10000, print_cost=True) # change lr and nh\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INcMFDI3xszV"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    \n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    predictions = (A2 > 0.5)   \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YOfLQElcxz5c",
    "outputId": "12e593f6-a874-46a3-f9f7-90d92114a1b4"
   },
   "outputs": [],
   "source": [
    "predictions = predict(parameters, X)\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "bvxWSLFCxtUf",
    "outputId": "ba5e145b-d4be-446f-e56c-bb2bcc1d7e67"
   },
   "outputs": [],
   "source": [
    "parameters = model(X, y, n_h = 4, num_iterations = 10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
